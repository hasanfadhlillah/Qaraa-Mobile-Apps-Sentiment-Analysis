{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a25c548",
   "metadata": {},
   "source": [
    "# **Proyek Analisis Sentimen Aplikasi \"Qara'a - Belajar Ngaji Quran\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92197a65",
   "metadata": {},
   "source": [
    "### BAGIAN 0: SETUP DAN IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Sastrawi (Stemming Bahasa Indonesia)\n",
    "%pip install sastrawi -q\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # Alternatif stopword remover\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB # Baik untuk teks & TF-IDF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# TensorFlow / Keras (Deep Learning)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, GlobalMaxPooling1D, Conv1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Mengatur seed untuk reproduktifitas\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Opsi Pandas\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "print(\"Semua library berhasil diimport.\")\n",
    "print(f\"Versi TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca64604",
   "metadata": {},
   "source": [
    "### BAGIAN 1: MEMUAT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- BAGIAN 1: MEMUAT DATASET ---\")\n",
    "# Pastikan file qaraa_reviews_scraped.csv ada di folder dataset_scraped/\n",
    "data_path = 'dataset_scraped/qaraa_reviews_scraped.csv'\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(data_path)\n",
    "    print(f\"Dataset berhasil dimuat: {data_path}\")\n",
    "    print(f\"Jumlah baris awal: {len(df_raw)}\")\n",
    "    print(f\"Jumlah kolom awal: {len(df_raw.columns)}\")\n",
    "    print(\"Informasi dataset awal:\")\n",
    "    df_raw.info()\n",
    "    print(\"\\nContoh data awal:\")\n",
    "    print(df_raw.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File dataset tidak ditemukan di {data_path}\")\n",
    "    print(\"Pastikan Anda sudah menjalankan notebook scraping (1_scraping_reviews.ipynb) dan file CSV sudah ada.\")\n",
    "    df_raw = pd.DataFrame() # Buat dataframe kosong agar notebook bisa jalan sebagian\n",
    "\n",
    "# Hanya gunakan kolom yang relevan, terutama 'content' untuk teks ulasan dan 'score' untuk pelabelan awal jika ada\n",
    "if not df_raw.empty:\n",
    "    df = df_raw[['content', 'score']].copy() # 'score' mungkin digunakan untuk validasi lexicon atau pelabelan awal\n",
    "    print(\"\\nDataset setelah memilih kolom 'content' dan 'score':\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    df = pd.DataFrame({'content': [], 'score': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fb0e6",
   "metadata": {},
   "source": [
    "### BAGIAN 2: EKSPLORASI DATA AWAL (EDA) & DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fd3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- BAGIAN 2: EDA & DATA CLEANING ---\")\n",
    "# Cek missing values\n",
    "print(\"\\nMissing values per kolom:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Hapus baris dengan 'content' yang kosong (jika ada)\n",
    "df.dropna(subset=['content'], inplace=True)\n",
    "df = df[df['content'].str.strip() != ''] # Hapus jika content hanya spasi\n",
    "print(f\"Jumlah baris setelah menghapus content kosong: {len(df)}\")\n",
    "\n",
    "# Cek duplikasi ulasan\n",
    "print(f\"Jumlah ulasan duplikat (berdasarkan 'content'): {df.duplicated(subset=['content']).sum()}\")\n",
    "df.drop_duplicates(subset=['content'], keep='first', inplace=True)\n",
    "print(f\"Jumlah baris setelah menghapus duplikat: {len(df)}\")\n",
    "\n",
    "# Distribusi 'score' (jika akan digunakan)\n",
    "if 'score' in df.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x='score', data=df, palette='viridis')\n",
    "    plt.title('Distribusi Skor Ulasan Awal')\n",
    "    plt.xlabel('Skor')\n",
    "    plt.ylabel('Jumlah Ulasan')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e054c",
   "metadata": {},
   "source": [
    "### BAGIAN 3: TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Fungsi-fungsi Preprocessing\n",
    "def cleaning_text(text):\n",
    "    text = str(text).lower() # Case folding awal\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text) # Hapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', ' ', text) # Hapus hashtag\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text) # Hapus URL\n",
    "    text = re.sub(r'\\d+', ' ', text) # Hapus angka\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Hapus tanda baca\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Hapus spasi berlebih\n",
    "    # Hapus karakter non-alfabet\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Kamus slang (bisa diperluas atau menggunakan file eksternal)\n",
    "slang_dict_path = 'https://raw.githubusercontent.com/louisowen6/NLP_bahasa_resources/master/combined_slang_words.txt'\n",
    "try:\n",
    "    response = requests.get(slang_dict_path)\n",
    "    slang_lines = response.text.split('\\n')\n",
    "    slang_dict = {}\n",
    "    for line in slang_lines:\n",
    "        if line.strip():\n",
    "            parts = line.split(':')\n",
    "            if len(parts) == 2:\n",
    "                slang_dict[parts[0].strip()] = parts[1].strip()\n",
    "    print(f\"Kamus slang berhasil dimuat dengan {len(slang_dict)} entri.\")\n",
    "except Exception as e:\n",
    "    print(f\"Gagal memuat kamus slang dari URL: {e}. Menggunakan kamus default kecil.\")\n",
    "    slang_dict = {\n",
    "        \"yg\": \"yang\", \"jg\": \"juga\", \"ga\": \"tidak\", \"gak\": \"tidak\", \"tdk\": \"tidak\",\n",
    "        \"gaada\": \"tidak ada\", \"ngga\": \"tidak\", \"nggak\": \"tidak\", \"kalo\": \"kalau\",\n",
    "        \"sih\": \"\", \"deh\": \"\", \"dong\": \"\", \"nih\": \"\", \"spt\": \"seperti\",\n",
    "        \"utk\": \"untuk\", \"bgmn\": \"bagaimana\", \"sdh\": \"sudah\", \"blm\": \"belum\",\n",
    "        \"bgt\": \"banget\", \"mantul\": \"mantap betul\", \"thx\": \"terima kasih\",\n",
    "        \"apk\": \"aplikasi\", \"app\": \"aplikasi\", \"min\": \"admin\", \"mimin\": \"admin\"\n",
    "    }\n",
    "\n",
    "def normalize_slang(text):\n",
    "    words = text.split()\n",
    "    normalized_words = [slang_dict.get(word, word) for word in words]\n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Stopwords (NLTK Indonesia + tambahan)\n",
    "stop_words_nltk = list(stopwords.words('indonesian'))\n",
    "additional_stopwords = [\n",
    "    'sih', 'deh', 'dong', 'nih', 'kak', 'ka', 'kk', 'gan', 'bro', 'sis', 'nya', 'iya', 'yaa', 'ya', 'yaa',\n",
    "    'aplikasi', 'aplikasinya', 'app', 'apps', 'qaraa', # kata terkait nama aplikasi\n",
    "    'saya', 'aku', 'gue', 'gw', 'gua', # kata ganti orang pertama sering muncul tapi kurang informatif untuk sentimen umum\n",
    "    'admin', 'mimin', 'developer', 'dev', # kata terkait pengembang\n",
    "    'tolong', 'mohon', 'bantu', # kata permintaan\n",
    "    'update', 'versi', # kata terkait versi aplikasi\n",
    "    'download', 'instal', 'install' # kata terkait instalasi\n",
    "]\n",
    "stop_words_combined = list(set(stop_words_nltk + additional_stopwords))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_combined and len(word) > 2] # Hapus kata pendek juga\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Stemming (Sastrawi)\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def stemming_text(text):\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "# 3.2. Menerapkan Preprocessing\n",
    "if not df.empty:\n",
    "    print(\"\\nMemulai proses preprocessing teks...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    df['text_cleaned'] = df['content'].apply(cleaning_text)\n",
    "    print(\"Cleaning selesai.\")\n",
    "    df['text_normalized'] = df['text_cleaned'].apply(normalize_slang)\n",
    "    print(\"Normalisasi slang selesai.\")\n",
    "    df['text_no_stopwords'] = df['text_normalized'].apply(remove_stopwords)\n",
    "    print(\"Stopword removal selesai.\")\n",
    "    # Stemming\n",
    "    df_processed = df[df['text_no_stopwords'].str.strip() != ''].copy()\n",
    "    if not df_processed.empty:\n",
    "        df_processed['text_stemmed'] = df_processed['text_no_stopwords'].apply(stemming_text)\n",
    "        print(\"Stemming selesai.\")\n",
    "        # Kolom final untuk analisis adalah 'text_stemmed'\n",
    "        df_processed['text_final'] = df_processed['text_stemmed']\n",
    "    else:\n",
    "        print(\"Tidak ada data setelah stopword removal, skipping stemming.\")\n",
    "        df_processed['text_final'] = \"\" # Atau handle error\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Preprocessing selesai dalam {end_time - start_time:.2f} detik.\")\n",
    "    print(\"\\nContoh data setelah preprocessing:\")\n",
    "    print(df_processed[['content', 'text_final']].head())\n",
    "\n",
    "    # Hapus baris yang text_final nya kosong setelah semua proses\n",
    "    df_processed.dropna(subset=['text_final'], inplace=True)\n",
    "    df_processed = df_processed[df_processed['text_final'].str.strip() != '']\n",
    "    print(f\"Jumlah baris setelah preprocessing dan cleaning akhir: {len(df_processed)}\")\n",
    "else:\n",
    "    print(\"\\nPreprocessing dilewati karena data kosong.\")\n",
    "    df_processed = pd.DataFrame({'content': [], 'text_final': [], 'score': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6a50e",
   "metadata": {},
   "source": [
    "### BAGIAN 4: PELABELAN SENTIMEN (3 KELAS: POSITIF, NEGATIF, NETRAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- BAGIAN 4: PELABELAN SENTIMEN ---\")\n",
    "# Kriteria Utama 2 terpenuhi: Melakukan tahapan pelabelan data\n",
    "# Saran 3 terpenuhi: Dataset memiliki tiga kelas\n",
    "\n",
    "# Menggunakan lexicon dari contoh\n",
    "lexicon_pos_url = 'https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv'\n",
    "lexicon_neg_url = 'https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv'\n",
    "\n",
    "# Membuat folder lexicons jika belum ada\n",
    "lexicon_folder = 'lexicons'\n",
    "if not os.path.exists(lexicon_folder):\n",
    "    os.makedirs(lexicon_folder)\n",
    "\n",
    "lexicon_positive_path = os.path.join(lexicon_folder, 'lexicon_positive.csv')\n",
    "lexicon_negative_path = os.path.join(lexicon_folder, 'lexicon_negative.csv')\n",
    "\n",
    "def download_lexicon(url, path):\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Lexicon berhasil diunduh dan disimpan di {path}\")\n",
    "            return True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Gagal mengunduh lexicon dari {url}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Lexicon sudah ada di {path}\")\n",
    "        return True\n",
    "\n",
    "# Unduh lexicon jika belum ada\n",
    "download_lexicon(lexicon_pos_url, lexicon_positive_path)\n",
    "download_lexicon(lexicon_neg_url, lexicon_negative_path)\n",
    "\n",
    "# Memuat lexicon ke dictionary\n",
    "lexicon_positive = {}\n",
    "lexicon_negative = {}\n",
    "\n",
    "try:\n",
    "    with open(lexicon_positive_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if len(row) == 2: lexicon_positive[row[0]] = int(row[1])\n",
    "    print(f\"Lexicon positif dimuat dengan {len(lexicon_positive)} entri.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File lexicon positif tidak ditemukan di {lexicon_positive_path}\")\n",
    "\n",
    "try:\n",
    "    with open(lexicon_negative_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if len(row) == 2: lexicon_negative[row[0]] = int(row[1])\n",
    "    print(f\"Lexicon negatif dimuat dengan {len(lexicon_negative)} entri.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File lexicon negatif tidak ditemukan di {lexicon_negative_path}\")\n",
    "\n",
    "def sentiment_lexicon_labeling(text):\n",
    "    score = 0\n",
    "    words = word_tokenize(str(text).lower()) # Tokenize lagi teks final (stemmed)\n",
    "    for word in words:\n",
    "        score += lexicon_positive.get(word, 0)\n",
    "        score += lexicon_negative.get(word, 0) # Skor negatif sudah negatif\n",
    "\n",
    "    # Aturan untuk 3 kelas\n",
    "    if score > 0: \n",
    "        return 'positive'\n",
    "    elif score < 0: \n",
    "        return 'negative'\n",
    "    else: # Skor 0 atau di antara threshold\n",
    "        return 'neutral'\n",
    "\n",
    "if not df_processed.empty and 'text_final' in df_processed.columns:\n",
    "    df_processed['sentiment_label'] = df_processed['text_final'].apply(sentiment_lexicon_labeling)\n",
    "    print(\"\\nPelabelan sentimen selesai.\")\n",
    "    print(\"Distribusi label sentimen:\")\n",
    "    print(df_processed['sentiment_label'].value_counts())\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x='sentiment_label', data=df_processed, palette='pastel', order=['positive', 'neutral', 'negative'])\n",
    "    plt.title('Distribusi Label Sentimen (3 Kelas)')\n",
    "    plt.xlabel('Sentimen')\n",
    "    plt.ylabel('Jumlah Ulasan')\n",
    "    plt.show()\n",
    "\n",
    "    # Encoding label menjadi numerik\n",
    "    label_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "    df_processed['sentiment_encoded'] = df_processed['sentiment_label'].map(label_mapping)\n",
    "    print(\"\\nContoh data dengan label sentimen:\")\n",
    "    print(df_processed[['text_final', 'sentiment_label', 'sentiment_encoded']].head())\n",
    "else:\n",
    "    print(\"\\nPelabelan dilewati karena data hasil preprocessing kosong.\")\n",
    "\n",
    "\n",
    "# Visualisasi WordCloud per sentimen\n",
    "if not df_processed.empty and 'sentiment_label' in df_processed.columns:\n",
    "    for sentiment_type in ['positive', 'neutral', 'negative']:\n",
    "        subset = df_processed[df_processed['sentiment_label'] == sentiment_type]\n",
    "        if not subset.empty:\n",
    "            text_concat = \" \".join(review for review in subset.text_final)\n",
    "            if text_concat.strip(): # Hanya buat wordcloud jika ada teks\n",
    "                wordcloud = WordCloud(width=800, height=400, background_color='white', collocations=False).generate(text_concat)\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(f'WordCloud untuk Sentimen: {sentiment_type.upper()}')\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Tidak ada teks untuk WordCloud sentimen {sentiment_type}\")\n",
    "        else:\n",
    "            print(f\"Tidak ada data untuk sentimen {sentiment_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427b698",
   "metadata": {},
   "source": [
    "### BAGIAN 5: PEMBAGIAN DATA & EKSTRAKSI FITUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kriteria Utama 2 terpenuhi: Melakukan tahapan ekstraksi fitur\n",
    "\n",
    "if not df_processed.empty and 'text_final' in df_processed.columns and 'sentiment_encoded' in df_processed.columns:\n",
    "    print(\"--- BAGIAN 5: PEMBAGIAN DATA & EKSTRAKSI FITUR ---\")\n",
    "    X = df_processed['text_final']\n",
    "    y = df_processed['sentiment_encoded']\n",
    "\n",
    "    # Pembagian data awal (akan divariasikan di skema)\n",
    "    X_train_main, X_test_main, y_train_main, y_test_main = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y) # stratify penting untuk data imbalance\n",
    "    print(f\"Ukuran X_train_main: {X_train_main.shape}\")\n",
    "    print(f\"Ukuran X_test_main: {X_test_main.shape}\")\n",
    "    print(f\"Ukuran y_train_main: {y_train_main.shape}\")\n",
    "    print(f\"Ukuran y_test_main: {y_test_main.shape}\")\n",
    "    print(\"\\nDistribusi kelas pada y_train_main:\")\n",
    "    print(y_train_main.value_counts(normalize=True))\n",
    "    print(\"\\nDistribusi kelas pada y_test_main:\")\n",
    "    print(y_test_main.value_counts(normalize=True))\n",
    "\n",
    "    # Ekstraksi Fitur: TF-IDF (untuk model ML Klasik)\n",
    "    # Parameter TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7, ngram_range=(1,2)) # ngram (1,2) untuk unigram & bigram\n",
    "\n",
    "    # Ekstraksi Fitur: Keras Tokenizer (untuk model Deep Learning)\n",
    "    # Parameter untuk Deep Learning\n",
    "    MAX_VOCAB_SIZE = 20000 # Jumlah kata unik maks dalam vocabulary\n",
    "    MAX_SEQUENCE_LENGTH = 200 # Panjang sekuens maks per ulasan (padding/truncating)\n",
    "    EMBEDDING_DIM = 100 # Dimensi embedding vector\n",
    "\n",
    "    # Menyiapkan Keras Tokenizer\n",
    "    keras_tokenizer = KerasTokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<oov>\") # <oov> untuk out-of-vocabulary words\n",
    "    # Fit tokenizer hanya pada data training utama untuk mencegah data leakage\n",
    "    keras_tokenizer.fit_on_texts(X_train_main)\n",
    "    word_index = keras_tokenizer.word_index\n",
    "    print(f\"\\nFound {len(word_index)} unique tokens in Keras Tokenizer.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nPembagian data dan ekstraksi fitur dilewati karena data tidak lengkap.\")\n",
    "\n",
    "# Fungsi bantuan untuk evaluasi model\n",
    "def evaluate_model(model_name, y_true_train, y_pred_train, y_true_test, y_pred_test):\n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_true_test, y_pred_test)\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"Akurasi Training: {train_acc*100:.2f}%\")\n",
    "    print(f\"Akurasi Testing: {test_acc*100:.2f}%\")\n",
    "    print(\"\\nLaporan Klasifikasi (Test Set):\")\n",
    "    print(classification_report(y_true_test, y_pred_test, target_names=['negative', 'neutral', 'positive'], zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "    plt.title(f'Confusion Matrix - {model_name} (Test Set)')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    return train_acc, test_acc\n",
    "\n",
    "# Dictionary untuk menyimpan hasil akurasi semua skema\n",
    "training_scheme_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2900a",
   "metadata": {},
   "source": [
    "### BAGIAN 6: PELATIHAN MODEL (3 SKEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kriteria Utama 3 terpenuhi: Menggunakan algoritma pelatihan machine learning\n",
    "# Saran 1 terpenuhi: Menggunakan algoritma deep learning (di Skema 1)\n",
    "# Saran 5 terpenuhi: Melakukan 3 percobaan skema pelatihan yang berbeda\n",
    "\n",
    "print(\"--- BAGIAN 6: PELATIHAN MODEL (3 SKEMA) ---\")\n",
    "\n",
    "if not df_processed.empty and 'X_train_main' in locals(): # Cek apakah data siap\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # SKEMA 1: Deep Learning (BiLSTM) - Target Akurasi Train & Test > 92% (Saran 2)\n",
    "    # Ekstraksi Fitur: Keras Embedding Layer\n",
    "    # Pembagian Data: 80% Train / 20% Test (dari X_train_main, y_train_main)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n--- SKEMA 1: Deep Learning (BiLSTM) ---\")\n",
    "    scheme_name_1 = \"Skema 1: BiLSTM (Embedding Keras, Split 80/20)\"\n",
    "\n",
    "    # Konversi teks ke sekuens angka\n",
    "    X_train_seq_s1 = keras_tokenizer.texts_to_sequences(X_train_main)\n",
    "    X_test_seq_s1 = keras_tokenizer.texts_to_sequences(X_test_main)\n",
    "\n",
    "    # Padding sekuens\n",
    "    X_train_pad_s1 = pad_sequences(X_train_seq_s1, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    X_test_pad_s1 = pad_sequences(X_test_seq_s1, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "    # One-hot encode labels untuk Keras (karena 3 kelas)\n",
    "    y_train_cat_s1 = to_categorical(y_train_main, num_classes=3)\n",
    "    y_test_cat_s1 = to_categorical(y_test_main, num_classes=3)\n",
    "\n",
    "    # Membangun model BiLSTM\n",
    "    model_bilstm = Sequential([\n",
    "        Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(3, activation='softmax') # 3 kelas output, softmax untuk probabilitas\n",
    "    ])\n",
    "\n",
    "    model_bilstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Coba learning rate berbeda\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "    print(\"\\nArsitektur Model BiLSTM:\")\n",
    "    model_bilstm.summary()\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1) # Patience bisa ditingkatkan\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1)\n",
    "\n",
    "    print(\"\\nMemulai pelatihan model BiLSTM...\")\n",
    "    history_bilstm = model_bilstm.fit(\n",
    "        X_train_pad_s1, y_train_cat_s1,\n",
    "        epochs=25,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_test_pad_s1, y_test_cat_s1),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Plot history pelatihan\n",
    "    pd.DataFrame(history_bilstm.history).plot(figsize=(10,6))\n",
    "    plt.gca().set_ylim(0,1) # Set y-axis from 0 to 1\n",
    "    plt.title(\"Riwayat Pelatihan Model BiLSTM\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluasi Model BiLSTM\n",
    "    y_pred_train_proba_s1 = model_bilstm.predict(X_train_pad_s1)\n",
    "    y_pred_train_s1 = np.argmax(y_pred_train_proba_s1, axis=1)\n",
    "    y_pred_test_proba_s1 = model_bilstm.predict(X_test_pad_s1)\n",
    "    y_pred_test_s1 = np.argmax(y_pred_test_proba_s1, axis=1)\n",
    "\n",
    "    train_acc_s1, test_acc_s1 = evaluate_model(scheme_name_1, y_train_main, y_pred_train_s1, y_test_main, y_pred_test_s1)\n",
    "    training_scheme_results[scheme_name_1] = {'train_acc': train_acc_s1, 'test_acc': test_acc_s1}\n",
    "\n",
    "    if train_acc_s1 > 0.92 and test_acc_s1 > 0.92:\n",
    "        print(f\"SELAMAT! {scheme_name_1} memenuhi target Saran 2 (Akurasi Train & Test > 92%).\")\n",
    "    elif test_acc_s1 >= 0.85:\n",
    "        print(f\"{scheme_name_1} memenuhi Kriteria Utama 4 (Akurasi Test >= 85%).\")\n",
    "    else:\n",
    "        print(f\"PERHATIAN! {scheme_name_1} TIDAK memenuhi Kriteria Utama 4. Perlu optimasi model lebih lanjut.\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # SKEMA 2: Machine Learning (Logistic Regression) - Target Akurasi Test > 85%\n",
    "    # Ekstraksi Fitur: TF-IDF\n",
    "    # Pembagian Data: 80% Train / 20% Test (dari X_train_main, y_train_main)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n--- SKEMA 2: Logistic Regression dengan TF-IDF ---\")\n",
    "    scheme_name_2 = \"Skema 2: Logistic Regression (TF-IDF, Split 80/20)\"\n",
    "\n",
    "    # Fit dan transform TF-IDF\n",
    "    X_train_tfidf_s2 = tfidf_vectorizer.fit_transform(X_train_main)\n",
    "    X_test_tfidf_s2 = tfidf_vectorizer.transform(X_test_main)\n",
    "    print(f\"Dimensi X_train_tfidf_s2: {X_train_tfidf_s2.shape}\")\n",
    "\n",
    "    # Model Logistic Regression\n",
    "    logreg_model = LogisticRegression(solver='liblinear', C=1.0, random_state=seed, multi_class='ovr', max_iter=1000)\n",
    "\n",
    "    print(\"\\nMemulai pelatihan model Logistic Regression...\")\n",
    "    logreg_model.fit(X_train_tfidf_s2, y_train_main)\n",
    "\n",
    "    # Evaluasi Model Logistic Regression\n",
    "    y_pred_train_s2 = logreg_model.predict(X_train_tfidf_s2)\n",
    "    y_pred_test_s2 = logreg_model.predict(X_test_tfidf_s2)\n",
    "\n",
    "    train_acc_s2, test_acc_s2 = evaluate_model(scheme_name_2, y_train_main, y_pred_train_s2, y_test_main, y_pred_test_s2)\n",
    "    training_scheme_results[scheme_name_2] = {'train_acc': train_acc_s2, 'test_acc': test_acc_s2}\n",
    "\n",
    "    if test_acc_s2 >= 0.85:\n",
    "        print(f\"{scheme_name_2} memenuhi Kriteria Utama 4 (Akurasi Test >= 85%).\")\n",
    "    else:\n",
    "        print(f\"PERHATIAN! {scheme_name_2} TIDAK memenuhi Kriteria Utama 4. Perlu optimasi model lebih lanjut.\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # SKEMA 3: Machine Learning (SVM) - Target Akurasi Test > 85%\n",
    "    # Ekstraksi Fitur: TF-IDF (bisa sama dengan Skema 2 atau divariasikan parameternya)\n",
    "    # Pembagian Data: 70% Train / 30% Test (BERBEDA dari skema sebelumnya)\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n--- SKEMA 3: SVM dengan TF-IDF dan Pembagian Data Berbeda ---\")\n",
    "    scheme_name_3 = \"Skema 3: SVM (TF-IDF, Split 70/30)\"\n",
    "\n",
    "    # Pembagian data baru untuk skema ini (70/30)\n",
    "    X_train_s3, X_test_s3, y_train_s3, y_test_s3 = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "    # Fit dan transform TF-IDF (bisa menggunakan tfidf_vectorizer yang sama atau baru)\n",
    "    tfidf_vectorizer_s3 = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.7, ngram_range=(1,2))\n",
    "    X_train_tfidf_s3 = tfidf_vectorizer_s3.fit_transform(X_train_s3)\n",
    "    X_test_tfidf_s3 = tfidf_vectorizer_s3.transform(X_test_s3)\n",
    "    print(f\"Dimensi X_train_tfidf_s3: {X_train_tfidf_s3.shape}\")\n",
    "\n",
    "    # Model SVM\n",
    "    svm_model = SVC(kernel='linear', C=1.0, random_state=seed, probability=True)\n",
    "\n",
    "    print(\"\\nMemulai pelatihan model SVM...\")\n",
    "    svm_model.fit(X_train_tfidf_s3, y_train_s3)\n",
    "\n",
    "    # Evaluasi Model SVM\n",
    "    y_pred_train_s3 = svm_model.predict(X_train_tfidf_s3)\n",
    "    y_pred_test_s3 = svm_model.predict(X_test_tfidf_s3)\n",
    "\n",
    "    train_acc_s3, test_acc_s3 = evaluate_model(scheme_name_3, y_train_s3, y_pred_train_s3, y_test_s3, y_pred_test_s3)\n",
    "    training_scheme_results[scheme_name_3] = {'train_acc': train_acc_s3, 'test_acc': test_acc_s3}\n",
    "\n",
    "    if test_acc_s3 >= 0.85:\n",
    "        print(f\"{scheme_name_3} memenuhi Kriteria Utama 4 (Akurasi Test >= 85%).\")\n",
    "    else:\n",
    "        print(f\"PERHATIAN! {scheme_name_3} TIDAK memenuhi Kriteria Utama 4. Perlu optimasi model lebih lanjut.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nPelatihan model dilewati karena data tidak siap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8549d2",
   "metadata": {},
   "source": [
    "### BAGIAN 7: PERBANDINGAN HASIL SKEMA PELATIHAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2be2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- BAGIAN 7: PERBANDINGAN HASIL SKEMA PELATIHAN ---\")\n",
    "if training_scheme_results:\n",
    "    results_df = pd.DataFrame.from_dict(training_scheme_results, orient='index')\n",
    "    results_df = results_df.sort_values(by='test_acc', ascending=False)\n",
    "    print(results_df)\n",
    "\n",
    "    # Cek pemenuhan Saran 5 (aturan akurasi)\n",
    "    num_schemes_above_85_test = sum(1 for res in training_scheme_results.values() if res['test_acc'] >= 0.85)\n",
    "    one_scheme_above_92_train_test = any(1 for res in training_scheme_results.values() if res['train_acc'] > 0.92 and res['test_acc'] > 0.92)\n",
    "\n",
    "    print(f\"\\nJumlah skema dengan akurasi test >= 85%: {num_schemes_above_85_test}\")\n",
    "    print(f\"Ada skema dengan akurasi train & test > 92%: {one_scheme_above_92_train_test}\")\n",
    "\n",
    "    if num_schemes_above_85_test >= 3:\n",
    "        print(\"Saran 5 (3 skema pelatihan) terpenuhi dari sisi jumlah skema dengan akurasi test >= 85%.\")\n",
    "        if one_scheme_above_92_train_test:\n",
    "             print(\"Saran 2 (Akurasi >92%) juga terpenuhi untuk setidaknya satu skema.\")\n",
    "        else:\n",
    "            print(\"Saran 2 (Akurasi >92%) TIDAK terpenuhi. Pastikan minimal satu skema mencapai target ini.\")\n",
    "    else:\n",
    "        print(\"PERHATIAN: Saran 5 (3 skema pelatihan) belum sepenuhnya terpenuhi dari sisi target akurasi untuk semua skema.\")\n",
    "        print(\"Pastikan setidaknya 3 skema memiliki akurasi test >= 85%.\")\n",
    "        print(\"Jika menargetkan Saran 2, pastikan satu skema >92% (train & test) dan sisanya >=85% (test).\")\n",
    "\n",
    "else:\n",
    "    print(\"Tidak ada hasil skema pelatihan untuk dibandingkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb499c5",
   "metadata": {},
   "source": [
    "### BAGIAN 8: INFERENCE DENGAN MODEL TERBAIK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ea62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saran 6 terpenuhi: Melakukan inference yang menghasilkan output kelas kategorikal\n",
    "\n",
    "print(\"--- BAGIAN 8: INFERENCE DENGAN MODEL TERBAIK ---\")\n",
    "\n",
    "def predict_sentiment_new_text(text, model_dl, keras_tokenizer_inf, label_map_inv):\n",
    "    # Preprocessing teks baru (sama seperti saat training)\n",
    "    cleaned = cleaning_text(text)\n",
    "    normalized = normalize_slang(cleaned)\n",
    "    no_stopwords = remove_stopwords(normalized)\n",
    "    stemmed = stemming_text(no_stopwords)\n",
    "    final_text = stemmed\n",
    "    print(f\"Teks setelah preprocessing: '{final_text}'\")\n",
    "\n",
    "    if not final_text.strip():\n",
    "        return \"Tidak ada teks untuk diprediksi setelah preprocessing.\", None\n",
    "\n",
    "    # Konversi ke sekuens dan padding (untuk model DL)\n",
    "    sequence = keras_tokenizer_inf.texts_to_sequences([final_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "    # Prediksi\n",
    "    prediction_proba = model_dl.predict(padded_sequence, verbose=0)\n",
    "    predicted_class_index = np.argmax(prediction_proba, axis=1)[0]\n",
    "    predicted_sentiment_label = label_map_inv.get(predicted_class_index, \"Tidak Diketahui\")\n",
    "    \n",
    "    return predicted_sentiment_label, prediction_proba[0]\n",
    "\n",
    "\n",
    "if 'model_bilstm' in locals() and 'keras_tokenizer' in locals(): # Cek model dan tokenizer ada\n",
    "    # Buat inverse mapping untuk label\n",
    "    inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "    contoh_ulasan_1 = \"Aplikasinya bagus banget, sangat membantu belajar Quran jadi mudah dan menyenangkan!\"\n",
    "    contoh_ulasan_2 = \"Banyak iklan yang mengganggu, jadi malas pakainya.\"\n",
    "    contoh_ulasan_3 = \"Lumayan sih, tapi fiturnya standar aja.\"\n",
    "    contoh_ulasan_4 = \"Error terus pas dibuka, tolong perbaiki dong developernya.\"\n",
    "    contoh_ulasan_5 = \"Alhamdulillah, anak saya jadi semangat ngaji pakai aplikasi ini.\"\n",
    "\n",
    "    list_ulasan_test = [contoh_ulasan_1, contoh_ulasan_2, contoh_ulasan_3, contoh_ulasan_4, contoh_ulasan_5]\n",
    "\n",
    "    print(\"\\nMelakukan inference pada beberapa contoh ulasan baru:\")\n",
    "    for i, ulasan in enumerate(list_ulasan_test):\n",
    "        print(f\"\\nUlasan {i+1}: \\\"{ulasan}\\\"\")\n",
    "        pred_label, pred_proba = predict_sentiment_new_text(ulasan, model_bilstm, keras_tokenizer, inverse_label_mapping)\n",
    "        print(f\"Prediksi Sentimen: {pred_label.upper()}\")\n",
    "        if pred_proba is not None:\n",
    "            print(f\"Probabilitas: Positif={pred_proba[inverse_label_mapping.get('positive',2)]:.2f}, Neutral={pred_proba[inverse_label_mapping.get('neutral',1)]:.2f}, Negatif={pred_proba[inverse_label_mapping.get('negative',0)]:.2f}\")\n",
    "        print(\"---\")\n",
    "    print(\"Screenshot bagian ini atau output cell ini sebagai bukti inferensi (Saran 6).\")\n",
    "else:\n",
    "    print(\"Inference dilewati karena model BiLSTM atau Keras Tokenizer tidak tersedia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947eeb72",
   "metadata": {},
   "source": [
    "### BAGIAN 9: KESIMPULAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc52a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
