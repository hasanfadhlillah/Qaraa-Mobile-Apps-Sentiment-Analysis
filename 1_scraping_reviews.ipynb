{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81867287",
   "metadata": {},
   "source": [
    "# **Scraping Ulasan Aplikasi \"Qara'a - Belajar Ngaji Quran\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40f453",
   "metadata": {},
   "source": [
    "## 1. Instalasi dan Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf298be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-play-scraper in d:\\anaconda\\envs\\dsml\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\dsml\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\envs\\dsml\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\anaconda\\envs\\dsml\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\dsml\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\dsml\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalasi dan Import Library\n",
    "%pip install google-play-scraper pandas\n",
    "\n",
    "from google_play_scraper import app, reviews_all, Sort\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d8347",
   "metadata": {},
   "source": [
    "* **Metode yang digunakan:**\n",
    "    * `%pip install`: Ini adalah *magic command* IPython (digunakan di Jupyter Notebook) untuk menjalankan perintah `pip` (Python package installer) langsung dari dalam cell. Perintah ini digunakan untuk menginstal dua pustaka Python:\n",
    "        * `google-play-scraper`: Pustaka utama untuk berinteraksi dengan Google Play Store dan mengambil data ulasan serta informasi aplikasi.\n",
    "        * `pandas`: Pustaka fundamental untuk manipulasi dan analisis data dalam Python, terutama untuk bekerja dengan data tabular seperti DataFrame.\n",
    "    * `from ... import ...`: Pernyataan standar Python untuk mengimpor modul atau objek spesifik dari pustaka yang telah diinstal agar bisa digunakan dalam kode.\n",
    "        * `app`, `reviews_all`, `Sort` dari `google_play_scraper`: Fungsi dan objek yang akan digunakan untuk mengambil semua ulasan aplikasi dan mengaturnya.\n",
    "        * `pandas as pd`: Mengimpor pustaka pandas dan memberinya alias `pd` (konvensi umum).\n",
    "        * `json`: Pustaka standar Python untuk bekerja dengan data format JSON (JavaScript Object Notation).\n",
    "        * `os`: Pustaka standar Python untuk berinteraksi dengan sistem operasi, seperti membuat direktori atau mengelola path file.\n",
    "\n",
    "* **Alasan penggunaan:**\n",
    "    * `google-play-scraper` dipilih karena merupakan alat yang efektif dan mudah digunakan untuk mengotomatisasi proses pengambilan data ulasan dari Google Play Store, yang menjadi sumber data utama proyek ini. Tanpa pustaka ini, pengambilan data secara manual akan sangat tidak efisien.\n",
    "    * `pandas` sangat penting untuk mengubah data ulasan yang diperoleh (biasanya dalam format list of dictionaries) menjadi struktur DataFrame yang terorganisir, sehingga memudahkan pembersihan, inspeksi, dan penyimpanan data.\n",
    "    * `json` digunakan untuk menyimpan data dalam format JSON, yang merupakan format data semi-terstruktur yang umum dan mudah dibaca baik oleh manusia maupun mesin. Ini memberikan alternatif format penyimpanan selain CSV.\n",
    "    * `os` diperlukan untuk manajemen file dan direktori secara programatik, seperti membuat folder `dataset_scraped` jika belum ada, untuk menyimpan hasil scraping.\n",
    "\n",
    "* **Insight dan Hasil yang didapat:**\n",
    "    * Output `Requirement already satisfied:` menunjukkan bahwa pustaka `google-play-scraper` dan `pandas` beserta dependensinya (seperti `python-dateutil`, `numpy`, `pytz`, `six`) sudah terinstal di environment Python yang digunakan. Jika belum terinstal, maka proses instalasi akan berjalan.\n",
    "    * Setelah cell ini dieksekusi, semua fungsi dan objek yang diimpor siap digunakan di cell-cell berikutnya dalam notebook. Ini adalah langkah persiapan esensial sebelum melakukan tugas inti yaitu scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e2b84",
   "metadata": {},
   "source": [
    "## 2. Konfigurasi Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f062eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi Scraping\n",
    "app_id = 'com.bismillah.amaljariyah'  # ID Aplikasi Qara'a\n",
    "target_reviews_count = 15000 # Target jumlah ulasan (minimal 10.000 untuk Saran 4)\n",
    "country_code = 'id'          # Kode negara (Indonesia)\n",
    "language_code = 'id'         # Kode bahasa (Indonesia)\n",
    "\n",
    "# Membuat folder untuk menyimpan dataset jika belum ada\n",
    "output_folder = 'dataset_scraped'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "csv_output_path = os.path.join(output_folder, 'qaraa_reviews_scraped.csv')\n",
    "json_output_path = os.path.join(output_folder, 'qaraa_reviews_scraped.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7eb32c",
   "metadata": {},
   "source": [
    "* **Metode yang digunakan:**\n",
    "    * Inisialisasi variabel: Mendefinisikan beberapa variabel konfigurasi yang akan digunakan dalam proses scraping dan penyimpanan data.\n",
    "    * Pemeriksaan dan pembuatan direktori: Menggunakan `os.path.exists()` untuk mengecek apakah folder tujuan sudah ada, dan `os.makedirs()` untuk membuat folder jika belum ada.\n",
    "    * Penggabungan path: Menggunakan `os.path.join()` untuk membuat path file output yang sesuai dengan sistem operasi (menangani separator `\\` atau `/` secara otomatis).\n",
    "\n",
    "* **Alasan penggunaan:**\n",
    "    * Variabel `app_id` (`'com.bismillah.amaljariyah'`) sangat krusial karena menentukan aplikasi mana yang ulasannya akan diambil. ID ini unik untuk setiap aplikasi di Google Play Store.\n",
    "    * `target_reviews_count` (`15000`) menetapkan jumlah ulasan yang diinginkan. Ini penting untuk memenuhi Kriteria Utama 1 (minimal 3.000 sampel) dan Saran 4 (minimal 10.000 sampel). Mengatur target yang lebih tinggi dari 10.000 memberikan margin jika tidak semua ulasan berhasil diambil atau jika ada duplikat yang perlu dihapus nanti.\n",
    "    * `country_code` (`'id'`) dan `language_code` (`'id'`) memastikan bahwa ulasan yang diambil berasal dari pengguna di Indonesia dan dalam Bahasa Indonesia, sesuai dengan konteks analisis yang mungkin diinginkan.\n",
    "    * Pengelolaan `output_folder`, `csv_output_path`, dan `json_output_path` dilakukan untuk mengorganisir file hasil scraping. Membuat folder secara otomatis jika belum ada meningkatkan kenyamanan pengguna dan menjaga kerapian proyek. `os.path.join` memastikan kompatibilitas path antar sistem operasi.\n",
    "        * *Catatan pada kode:* Komentar `# ID Aplikasi Qara'a` dan nama file output `qaraa_reviews_scraped.csv` sebaiknya disesuaikan jika `app_id` merujuk ke aplikasi lain (seperti dalam kasus ini `com.bismillah.amaljariyah`) untuk konsistensi.\n",
    "\n",
    "* **Insight dan Hasil yang didapat:**\n",
    "    * Cell ini tidak menghasilkan output visual, tetapi secara internal ia menyiapkan semua parameter yang diperlukan untuk proses scraping.\n",
    "    * Setelah eksekusi, variabel-variabel tersebut akan tersimpan dalam memori notebook.\n",
    "    * Jika folder `dataset_scraped` belum ada di direktori tempat notebook dijalankan, folder tersebut akan dibuat.\n",
    "    * Path lengkap untuk file output CSV dan JSON (misalnya `dataset_scraped\\qaraa_reviews_scraped.csv`) telah ditentukan dan siap digunakan untuk menyimpan data.\n",
    "    * Konfigurasi ini menunjukkan perencanaan yang baik untuk pengumpulan data, dengan memperhatikan target jumlah data dan spesifikasi regional/bahasa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601aae5",
   "metadata": {},
   "source": [
    "## 3. Proses Scraping dan Menyimpan Hasil Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92472967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai scraping ulasan untuk aplikasi: com.bismillah.amaljariyah\n",
      "Target jumlah ulasan: 15000\n",
      "Berhasil mengambil 11366 ulasan.\n",
      "\n",
      "Berhasil membuat DataFrame dengan 11366 baris dan 11 kolom.\n",
      "\n",
      "Contoh 5 ulasan pertama:\n",
      "                               reviewId         userName  \\\n",
      "0  9551cfa2-4552-4b55-baa8-073880f214d2  Pengguna Google   \n",
      "1  3df7b8be-e97b-41c0-b79b-71704e390519  Pengguna Google   \n",
      "2  a3fd4595-b2fe-4b13-8723-6e72bcd22e8a  Pengguna Google   \n",
      "3  eff8cb1e-86b4-4e42-a7a8-0c876c2dc813  Pengguna Google   \n",
      "4  b9d545ed-567b-40b3-aec2-15067d426d6a  Pengguna Google   \n",
      "\n",
      "                                           userImage  \\\n",
      "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
      "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
      "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
      "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
      "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
      "\n",
      "                                             content  score  thumbsUpCount  \\\n",
      "0  Tidak bisa d gunakan karena apa yg d pelajari ...      1              0   \n",
      "1  sebenarnya mau kasi bintang 5 tapi, aplikasi i...      4              0   \n",
      "2  bagus bgttt aplikasi nya memudahkan belajar Al...      5              0   \n",
      "3                                      Alhamdulillah      5              0   \n",
      "4                  sangat membantu pengetahuan agama      5              0   \n",
      "\n",
      "  reviewCreatedVersion                  at  \\\n",
      "0                 None 2025-05-27 06:40:13   \n",
      "1               5.1.24 2025-05-26 06:29:59   \n",
      "2               5.1.24 2025-05-26 06:10:32   \n",
      "3               5.1.24 2025-05-25 09:44:58   \n",
      "4               5.1.22 2025-05-23 18:07:12   \n",
      "\n",
      "                                        replyContent           repliedAt  \\\n",
      "0  Jazaka Allah khair, Sahabat, atas masukannya y... 2025-05-27 12:41:49   \n",
      "1  Masya Allah, senang banget dengar Qaraâ€™a bisa ... 2025-05-27 10:23:14   \n",
      "2  Masya Allah, terima kasih banyak ya sahabat! S... 2025-05-27 10:22:24   \n",
      "3  Alhamdulillah, sahabat! Terima kasih sudah bel... 2025-05-27 10:21:58   \n",
      "4  Masya Allah, terima kasih banyak atas ulasanny... 2025-05-27 10:21:27   \n",
      "\n",
      "  appVersion  \n",
      "0       None  \n",
      "1     5.1.24  \n",
      "2     5.1.24  \n",
      "3     5.1.24  \n",
      "4     5.1.22  \n",
      "\n",
      "Ulasan berhasil disimpan dalam format CSV di: dataset_scraped\\qaraa_reviews_scraped.csv\n",
      "Ulasan berhasil disimpan dalam format JSON di: dataset_scraped\\qaraa_reviews_scraped.json\n",
      "\n",
      "Proses scraping selesai.\n",
      "Jumlah ulasan yang di-scrape: 11366\n",
      "Jumlah ulasan memenuhi target minimal 10.000 sampel untuk Saran 4.\n"
     ]
    }
   ],
   "source": [
    "# Proses Scraping\n",
    "print(f\"Memulai scraping ulasan untuk aplikasi: {app_id}\")\n",
    "print(f\"Target jumlah ulasan: {target_reviews_count}\")\n",
    "\n",
    "try:\n",
    "    scraped_reviews = reviews_all(\n",
    "        app_id,\n",
    "        lang=language_code,\n",
    "        country=country_code,\n",
    "        sort=Sort.NEWEST, # Mengambil ulasan terbaru untuk variasi\n",
    "        count=target_reviews_count # Parameter count di reviews_all adalah batas, bisa jadi kurang dari ini\n",
    "    )\n",
    "    print(f\"Berhasil mengambil {len(scraped_reviews)} ulasan.\")\n",
    "\n",
    "    if not scraped_reviews:\n",
    "        print(\"Tidak ada ulasan yang berhasil diambil. Periksa kembali App ID atau koneksi internet.\")\n",
    "    else:\n",
    "        # Membuat DataFrame\n",
    "        df_reviews = pd.DataFrame(scraped_reviews)\n",
    "        print(f\"\\nBerhasil membuat DataFrame dengan {df_reviews.shape[0]} baris dan {df_reviews.shape[1]} kolom.\")\n",
    "        print(\"\\nContoh 5 ulasan pertama:\")\n",
    "        print(df_reviews.head())\n",
    "\n",
    "        # Menyimpan Hasil Scraping ke CSV\n",
    "        df_reviews.to_csv(csv_output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nUlasan berhasil disimpan dalam format CSV di: {csv_output_path}\")\n",
    "\n",
    "        # Menyimpan ke JSON (opsional, tapi baik untuk data semi-terstruktur)\n",
    "        # Mengubah DataFrame ke format list of dicts untuk JSON yang lebih rapi\n",
    "        df_for_json = df_reviews.copy()\n",
    "        if 'at' in df_for_json.columns:\n",
    "            df_for_json['at'] = df_for_json['at'].astype(str)\n",
    "        if 'repliedAt' in df_for_json.columns:\n",
    "            df_for_json['repliedAt'] = df_for_json['repliedAt'].astype(str)\n",
    "\n",
    "        reviews_json_data = df_for_json.to_dict(orient='records')\n",
    "        with open(json_output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(reviews_json_data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Ulasan berhasil disimpan dalam format JSON di: {json_output_path}\")\n",
    "\n",
    "        print(\"\\nProses scraping selesai.\")\n",
    "        print(f\"Jumlah ulasan yang di-scrape: {len(df_reviews)}\")\n",
    "        if len(df_reviews) >= 10000:\n",
    "            print(\"Jumlah ulasan memenuhi target minimal 10.000 sampel untuk Saran 4.\")\n",
    "        elif len(df_reviews) >= 3000:\n",
    "            print(\"Jumlah ulasan memenuhi target minimal 3.000 sampel untuk Kriteria Utama 1.\")\n",
    "        else:\n",
    "            print(\"PERHATIAN: Jumlah ulasan kurang dari 3.000. Mungkin perlu menjalankan scraping lagi atau mencoba mengambil dari sumber lain jika ulasan aplikasi ini terbatas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat scraping: {e}\")\n",
    "    print(\"Pastikan App ID benar dan memiliki koneksi internet yang stabil.\")\n",
    "    print(\"Google Play Scraper terkadang memiliki batasan jumlah request, coba lagi nanti jika perlu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fcd21",
   "metadata": {},
   "source": [
    "* **Metode yang digunakan:**\n",
    "    * `reviews_all()`: Fungsi inti dari `google-play-scraper` yang digunakan untuk mengambil semua ulasan yang cocok dengan parameter yang diberikan (`app_id`, `lang`, `country`, `sort`, `count`).\n",
    "        * `sort=Sort.NEWEST`: Mengurutkan ulasan berdasarkan yang terbaru. Ini baik untuk mendapatkan data yang paling relevan dengan kondisi aplikasi saat ini.\n",
    "        * `count=target_reviews_count`: Menentukan jumlah maksimum ulasan yang ingin diambil. Pustaka mungkin mengembalikan kurang dari ini jika jumlah ulasan yang tersedia lebih sedikit.\n",
    "    * `pd.DataFrame()`: Mengonversi list of dictionaries yang dikembalikan oleh `reviews_all()` menjadi DataFrame pandas.\n",
    "    * `df_reviews.head()`: Menampilkan lima baris pertama dari DataFrame untuk inspeksi cepat.\n",
    "    * `df_reviews.to_csv()`: Menyimpan DataFrame ke file CSV.\n",
    "        * `index=False`: Mencegah pandas menuliskan indeks DataFrame sebagai kolom di file CSV.\n",
    "        * `encoding='utf-8-sig'`: Menggunakan encoding UTF-8 dengan BOM (Byte Order Mark), yang membantu beberapa program (seperti Excel) untuk mengenali karakter non-ASCII dengan benar.\n",
    "    * Konversi Tipe Data untuk JSON: Kolom `at` dan `repliedAt` yang bertipe `Timestamp` dikonversi menjadi string (`.astype(str)`) sebelum disimpan ke JSON untuk menghindari error serialisasi.\n",
    "    * `df_for_json.to_dict(orient='records')`: Mengonversi DataFrame menjadi list of dictionaries, format yang ideal untuk JSON.\n",
    "    * `json.dump()`: Menyimpan data Python (list of dictionaries) ke file JSON.\n",
    "        * `ensure_ascii=False`: Memungkinkan karakter non-ASCII (seperti dalam Bahasa Indonesia) ditulis apa adanya tanpa di-escape.\n",
    "        * `indent=4`: Membuat format file JSON lebih mudah dibaca (pretty-print) dengan indentasi 4 spasi.\n",
    "    * Blok `try-except Exception as e`: Digunakan untuk menangani potensi error yang mungkin terjadi selama proses scraping (misalnya, ID aplikasi salah, tidak ada koneksi internet, batasan dari Google Play).\n",
    "\n",
    "* **Alasan penggunaan:**\n",
    "    * Fungsi `reviews_all()` dipilih karena kemampuannya untuk mengambil sejumlah besar ulasan secara otomatis, yang merupakan inti dari Kriteria 1. Pengurutan berdasarkan `Sort.NEWEST` memastikan data yang didapat adalah yang paling baru dan relevan.\n",
    "    * DataFrame pandas digunakan karena kemudahannya dalam mengelola dan memanipulasi data tabular.\n",
    "    * Penyimpanan ke CSV adalah standar untuk dataset tabular dan mudah diimpor oleh berbagai alat analisis data. Encoding `utf-8-sig` dipilih untuk kompatibilitas.\n",
    "    * Penyimpanan ke JSON bersifat opsional tetapi memberikan fleksibilitas jika struktur data lebih kompleks atau jika akan digunakan oleh sistem lain yang lebih menyukai JSON. Konversi tipe data `Timestamp` ke string sebelum serialisasi JSON adalah praktik yang baik untuk menghindari error.\n",
    "    * Penanganan error (`try-except`) penting untuk membuat skrip lebih robust dan memberikan pesan yang informatif jika terjadi masalah, daripada skrip berhenti tiba-tiba.\n",
    "\n",
    "* **Insight dan Hasil yang didapat:**\n",
    "    * Output menunjukkan bahwa proses scraping untuk aplikasi `com.bismillah.amaljariyah` berhasil dimulai.\n",
    "    * **Berhasil mengambil 11.366 ulasan**, yang melebihi target `target_reviews_count` yang diset menjadi `15000`. Ini adalah hasil yang sangat baik dan menunjukkan bahwa aplikasi tersebut memiliki banyak ulasan.\n",
    "        * *Catatan:* Parameter `count` di `reviews_all` adalah batas atas; jika aplikasi memiliki ulasan lebih sedikit dari `count`, maka semua ulasan yang tersedia (hingga batas internal pustaka jika ada) akan diambil. Jika aplikasi memiliki lebih banyak ulasan dari `count`, maka `count` akan menjadi batasnya. Dalam kasus ini, sepertinya aplikasi memiliki setidaknya 11.366 ulasan yang bisa diambil dengan parameter tersebut.\n",
    "    * DataFrame berhasil dibuat dengan **11.366 baris dan 11 kolom**, sesuai dengan jumlah ulasan yang diambil. Kolom-kolomnya (`reviewId`, `userName`, `userImage`, `content`, `score`, `thumbsUpCount`, `reviewCreatedVersion`, `at`, `replyContent`, `repliedAt`, `appVersion`) adalah standar dari `google-play-scraper` dan menyediakan informasi yang kaya.\n",
    "    * Contoh 5 ulasan pertama (`df_reviews.head()`) berhasil ditampilkan, memberikan gambaran data yang diperoleh. Terlihat ada ulasan dengan berbagai skor dan beberapa memiliki balasan dari pengembang.\n",
    "    * File CSV (`dataset_scraped\\qaraa_reviews_scraped.csv`) dan file JSON (`dataset_scraped\\qaraa_reviews_scraped.json`) **berhasil disimpan**. Ini penting karena file-file ini akan menjadi input untuk tahap analisis selanjutnya.\n",
    "    * Pesan konfirmasi \"Jumlah ulasan memenuhi target minimal 10.000 sampel untuk Saran 4\" menunjukkan bahwa **Kriteria Utama 1 dan Saran 4 terkait jumlah data telah terpenuhi**. Ini adalah pencapaian penting untuk proyek."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed4515",
   "metadata": {},
   "source": [
    "## Kesimpulan Akhir\n",
    "\n",
    "Secara keseluruhan **berhasil dengan sangat baik** dalam menjalankan tugas utamanya, yaitu **mengumpulkan dataset ulasan aplikasi dari Google Play Store** untuk aplikasi dengan ID `com.bismillah.amaljariyah`.\n",
    "\n",
    "**Poin-Poin Kunci Keberhasilan:**\n",
    "\n",
    "1.  **Persiapan Lingkungan:** Semua pustaka yang diperlukan (`google-play-scraper`, `pandas`, `json`, `os`) berhasil diimpor dan siap digunakan. Output menunjukkan bahwa pustaka-pustaka tersebut sudah terinstal dengan benar.\n",
    "2.  **Konfigurasi yang Tepat:** Parameter-parameter penting untuk proses scraping, seperti `app_id`, `target_reviews_count`, `country_code`, `language_code`, serta path untuk penyimpanan output, telah didefinisikan dengan jelas. Folder output juga berhasil dibuat jika belum ada.\n",
    "3.  **Proses Scraping Sukses:**\n",
    "    * **Pengambilan Data:** Fungsi `reviews_all` berhasil mengambil **11.366 ulasan**, yang merupakan jumlah yang substansial dan melebihi target minimal 10.000 ulasan (memenuhi Kriteria Utama 1 dan Saran 4).\n",
    "    * **Pembuatan DataFrame:** Data ulasan berhasil dikonversi menjadi DataFrame pandas dengan 11.366 baris dan 11 kolom, menyediakan struktur data yang terorganisir dan kaya informasi.\n",
    "    * **Penyimpanan Data:** DataFrame berhasil disimpan ke dalam format **CSV** (`dataset_scraped\\qaraa_reviews_scraped.csv`) dan **JSON** (`dataset_scraped\\qaraa_reviews_scraped.json`) setelah penanganan konversi tipe data `Timestamp` untuk JSON. Kedua format ini memberikan fleksibilitas untuk penggunaan data di tahap selanjutnya.\n",
    "    * **Penanganan Error:** Penggunaan blok `try-except` memastikan skrip berjalan dengan stabil dan memberikan informasi jika terjadi kendala.\n",
    "\n",
    "**Implikasi untuk Proyek:**\n",
    "\n",
    "* Dataset yang dihasilkan (11.366 ulasan) sudah sangat memadai untuk melanjutkan ke tahap preprocessing, pelabelan, pelatihan model, dan evaluasi dalam proyek analisis sentimen ini.\n",
    "* Kualitas data yang diambil (termasuk konten ulasan, skor, tanggal, dll.) memberikan dasar yang kuat untuk analisis yang mendalam.\n",
    "* Pemenuhan Kriteria Utama 1 (data scraping mandiri) dan Saran 4 (minimal 10.000 sampel) dari panduan proyek telah tercapai pada tahap ini."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
